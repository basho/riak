[
 {riak_core,
  [
   %% The cluster manager will listen for connections from remote
   %% clusters on this ip and port. Every node runs one cluster
   %% manager, but only the cluster manager running on the
   %% cluster_leader will service requests. This can change as nodes
   %% enter and leave the cluster.
   {cluster_mgr, {"{{cluster_manager_ip}}", {{cluster_manager_port}} } }
  ]},

 {riak_repl,
  [
   %% Path (relative or absolute) to the working directory for the
   %% replication process
   {data_root, "{{repl_data_root}}"},

   %% The hard limit of fullsync workers that will be running on the
   %% source side of a cluster across all nodes on that cluster for a
   %% fullsync to a sink cluster. This means if one has configured
   %% fullsync for two different clusters, both with a
   %% max_fssource_cluster of 5, 10 fullsync workers can be in
   %% progress. Only affects nodes on the source cluster on which this
   %% parameter is defined via the configuration file or command line.
   {max_fssource_cluster, 5},

   %% Limits the number of fullsync workers that will be running on
   %% each individual node in a source cluster. This is a hard limit for
   %% all fullsyncs enabled; additional fullsync configurations will not
   %% increase the number of fullsync workers allowed to run on any node.
   %% Only affects nodes on the source cluster on which this parameter is
   %% defined via the configuration file or command line.
   {max_fssource_node, 1},

   %% Limits the number of fullsync workers allowed to run on each
   %% individual node in a sink cluster. This is a hard limit for all
   %% fullsync sources interacting with the sink cluster. Thus, multiple
   %% simultaneous source connections to the sink cluster will have to
   %% share the sink node's number of maximum connections. Only affects
   %% nodes on the sink cluster on which this parameter is defined via
   %% the configuration file or command line.
   {max_fssink_node, 1},

   %% Whether to initiate a fullsync on initial connection from the
   %% sink cluster.
   {fullsync_on_connect, true},

   %% A single integer value representing the duration to wait in
   %% minutes between fullsyncs, or a list of {clustername,
   %% time_in_minutes} pairs for each sink participating in fullsync
   %% replication.
   {fullsync_interval, 30},

   %% The maximum size the realtime replication queue can grow to
   %% before new objects are dropped. Defaults to 100MB. Dropped
   %% objects will need to be replication with a fullsync.
   {rtq_max_bytes, 104857600},

   %% Enable Riak CS proxy_get and block filter.
   {proxy_get, disabled},

   %% A heartbeat message is sent from the source to the sink every
   %% heartbeat_interval. Setting heartbeat_interval to undefined
   %% disables the realtime heartbeat. This feature is only available in
   %% Riak Enterprise 1.3.2+.
   {rt_heartbeat_interval, 15},

   %% If a heartbeat response is not received in rt_heartbeat_timeout
   %% seconds, then the source connection exits and will be
   %% re-established. This feature is only available in Riak
   %% Enterprise 1.3.2+.
   {rt_heartbeat_timeout, 15},

   %% By default, fullsync replication will try to coordinate with
   %% other riak subsystems that may be contending for the same
   %% resources. This will help to prevent system response degradation
   %% under times of heavy load from multiple background tasks. To
   %% disable background coordination, set this parameter to false.
   %% Enterprise 2.0+.
   {fullsync_use_background_manager, true}
  ]},
  
  {lager,
   [
      {extra_sinks,
           [
            {object_lager_event,
             [{handlers,
               [{lager_file_backend,
                 [{file, "{{platform_log_dir}}/object.log"},
                  {level, info},
                  {formatter_config, [date, " ", time," [",severity,"] ",message, "\n"]}
                 ]
                }]
              },
              {async_threshold, 500},
              {async_threshold_window, 50}]
            }
            ]
      }
    ]
}
].
